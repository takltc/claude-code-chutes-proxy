version: "3.9"

services:
  proxy:
    build:
      context: .
      dockerfile: Dockerfile
    image: claude-chutes-proxy:local
    container_name: claude-chutes-proxy
    environment:
      # Service port inside the container
      - PORT=8080
      # Upstream (OpenAI-compatible) base URL
      - CHUTES_BASE_URL=${CHUTES_BASE_URL:-https://llm.chutes.ai}
      # Optional upstream API key; omit to forward inbound headers instead
      - CHUTES_API_KEY=${CHUTES_API_KEY:-}
      # Optional behavior flags
      - AUTO_FIX_MODEL_CASE=${AUTO_FIX_MODEL_CASE:-1}
      - DEBUG_PROXY=${DEBUG_PROXY:-0}
      - PROXY_BACKOFF_ON_429=${PROXY_BACKOFF_ON_429:-1}
      - PROXY_MAX_RETRY_ON_429=${PROXY_MAX_RETRY_ON_429:-1}
      - PROXY_MAX_RETRY_AFTER=${PROXY_MAX_RETRY_AFTER:-2}
      - CHUTES_AUTH_STYLE=${CHUTES_AUTH_STYLE:-both}
      # Optional model and tool name mapping JSON strings
      - MODEL_MAP=${MODEL_MAP:-{}}
      - TOOL_NAME_MAP=${TOOL_NAME_MAP:-{}}
    # Map host 8090 to container 8080 to avoid changing scripts
    ports:
      - "8090:8080"
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://localhost:${PORT}/ || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 3
    restart: unless-stopped
    # Uncomment to load variables from a local .env file
    # env_file:
    #   - .env

